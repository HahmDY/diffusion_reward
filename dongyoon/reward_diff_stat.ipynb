{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dongyoon/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import hydra\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import yaml\n",
    "from types import SimpleNamespace\n",
    "\n",
    "from diffusion_reward.models.video_models.vqdiffusion.modeling.build import build_model\n",
    "from diffusion_reward.models.video_models.vqdiffusion.modeling.transformers.diffusion_transformer import (\n",
    "    index_to_log_onehot, log_categorical, log_onehot_to_index,\n",
    "    sum_except_batch)\n",
    "from diffusion_reward.models.video_models.vqdiffusion.utils.io import load_yaml_config\n",
    "from diffusion_reward.models.video_models.vqdiffusion.utils.misc import get_model_parameters_info\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_DiffusionReward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.info = self.get_model(ema=True, model_path=cfg.ckpt_path, config_path=cfg.cfg_path)\n",
    "        self.model = self.info['model']\n",
    "        self.epoch = self.info['epoch']\n",
    "        self.model_name = self.info['model_name']\n",
    "        self.model.eval()\n",
    "        \n",
    "        for param in self.model.parameters(): \n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # set attribute\n",
    "        for attr_name, attr_value in vars(cfg).items():\n",
    "            setattr(self, attr_name, attr_value)\n",
    "        \n",
    "    def get_model(self, ema, model_path, config_path):\n",
    "        if 'OUTPUT' in model_path: # pretrained model\n",
    "            model_name = model_path.split(os.path.sep)[-3] # model_name으로 끝나는 경로\n",
    "        else: \n",
    "            model_name = os.path.basename(config_path).replace('.yaml', '')\n",
    "            \n",
    "        config = load_yaml_config(config_path)\n",
    "\n",
    "        model = build_model(config)\n",
    "        model_parameters = get_model_parameters_info(model)\n",
    "        \n",
    "        print(model_parameters)\n",
    "        if os.path.exists(model_path):\n",
    "            ckpt = torch.load(model_path, map_location=\"cpu\")\n",
    "            if 'last_epoch' in ckpt:\n",
    "                epoch = ckpt['last_epoch']\n",
    "            elif 'epoch' in ckpt:\n",
    "                epoch = ckpt['epoch']\n",
    "            else:\n",
    "                epoch = 0\n",
    "\n",
    "            missing, unexpected = model.load_state_dict(ckpt[\"model\"], strict=False)\n",
    "            print('Model missing keys:\\n', missing)\n",
    "            print('Model unexpected keys:\\n', unexpected)\n",
    "\n",
    "            if ema==True and 'ema' in ckpt:\n",
    "                print(\"Evaluate EMA model\")\n",
    "                ema_model = model.get_ema_model()\n",
    "                missing, unexpected = ema_model.load_state_dict(ckpt['ema'], strict=False)\n",
    "        else:\n",
    "            epoch = None\n",
    "        return {'model': model, 'epoch': epoch, 'model_name': model_name, 'parameter': model_parameters}\n",
    "    \n",
    "    def imgs_to_batch(self, x, reward_type='entropy'):\n",
    "        '''\n",
    "        input:\n",
    "            imgs: B * T * H * W * C\n",
    "            (mostly): 1 * T * ...\n",
    "        '''\n",
    "        assert x.max() <= 1\n",
    "        # preprocessing\n",
    "        seq_len = x.shape[1]\n",
    "        num_frames = self.model.cfg.params['condition_emb_config']['params']['num_cond_frames']\n",
    "        n_skip = self.model.frame_skip\n",
    "        subseq_len = (num_frames + 1) * n_skip\n",
    "\n",
    "        x = x.permute(0, 1, 4, 2 ,3) # B * T * H * W * C -> B * T * C * H * W\n",
    "        _, indices = self.model.content_codec.encode_to_z(x)\n",
    "        assert indices.shape[0] == 1\n",
    "        indices = indices.reshape(indices.shape[0], seq_len, -1)\n",
    "\n",
    "        if reward_type == 'entropy':\n",
    "            # only return conditional frames\n",
    "            post_idxes = list(range(seq_len - subseq_len + 2))\n",
    "            batch_indices = [indices[:, idx:idx+subseq_len-n_skip:n_skip] for idx in post_idxes]\n",
    "            batch_indices = torch.stack(batch_indices, dim=0)\n",
    "            batch_indices = batch_indices.squeeze(1).reshape(batch_indices.shape[0], -1)    \n",
    "            \n",
    "            if subseq_len - 2 > 0:\n",
    "                pre_batch_indices = [indices[:, idx].tile((1, num_frames)) for idx in range(subseq_len-2)]\n",
    "                pre_batch_indices = torch.concat(pre_batch_indices, dim=0)\n",
    "                batch_indices = torch.concat([pre_batch_indices, batch_indices], dim=0)\n",
    "            cond = {'condition_token': batch_indices}\n",
    "        elif reward_type == 'likelihood':\n",
    "            # return conditional frames + current frame\n",
    "            post_idxes = list(range(seq_len - subseq_len + 1))\n",
    "            batch_indices = [indices[:, idx:idx+subseq_len-n_skip:n_skip] for idx in post_idxes]\n",
    "            batch_indices = torch.stack(batch_indices, dim=0)\n",
    "            batch_indices = batch_indices.squeeze(1).reshape(batch_indices.shape[0], -1)    \n",
    "            \n",
    "            if subseq_len - 2 > 0:\n",
    "                pre_batch_indices = [indices[:, idx].tile((1, num_frames)) for idx in range(subseq_len-1)]\n",
    "                pre_batch_indices = torch.concat(pre_batch_indices, dim=0)\n",
    "                batch_indices = torch.concat([pre_batch_indices, batch_indices], dim=0)\n",
    "            cond = {'condition_token': batch_indices}\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        x = x.flatten(0, 1)\n",
    "        cont = {'content_token': indices[0]}\n",
    "        return cont, cond, indices[0]\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_reward(self, imgs):\n",
    "        self.model.eval()\n",
    "        content, condition, _ = self.imgs_to_batch(imgs, reward_type=self.reward_type)\n",
    "        content_token = content['content_token']\n",
    "        condition_token = condition['condition_token']\n",
    "\n",
    "        rewards = self.calc_vlb(content_token, condition_token)\n",
    "        if self.use_std:\n",
    "            rewards_std = (rewards - self.stat[0]) / self.stat[1]\n",
    "            scaled_rewards = (1 - self.expl_scale) * rewards_std\n",
    "            return scaled_rewards  \n",
    "        else:\n",
    "            return rewards\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def calc_vlb(self, cont_emb, cond_emb):\n",
    "        x = cont_emb\n",
    "        b, device = x.size(0), x.device\n",
    "        transformer = self.model.transformer\n",
    "        cond_emb = transformer.condition_emb(cond_emb).float()\n",
    "\n",
    "        # t=0\n",
    "        start_step = transformer.num_timesteps\n",
    "        x_start = x\n",
    "        t = torch.full((b,), start_step-1, device=device, dtype=torch.long)\n",
    "        log_x_start = index_to_log_onehot(x_start, transformer.num_classes)\n",
    "\n",
    "        # t=T\n",
    "        zero_logits = torch.zeros((b, transformer.num_classes-1, transformer.shape),device=device)\n",
    "        one_logits = torch.ones((b, 1, transformer.shape),device=device)\n",
    "        mask_logits = torch.cat((zero_logits, one_logits), dim=1)\n",
    "        log_z = torch.log(mask_logits)\n",
    "\n",
    "        # denoised time_steps\n",
    "        diffusion_list = [index for index in range(start_step-1, -1, -1-self.skip_step)]\n",
    "        if diffusion_list[-1] != 0:\n",
    "            diffusion_list.append(0)\n",
    "\n",
    "        vlbs = []\n",
    "        if self.reward_type == 'entropy':\n",
    "            # use denoised samples for estimation\n",
    "            for _ in range(self.num_sample):\n",
    "                start_step = transformer.num_timesteps\n",
    "                x_start = x\n",
    "                t = torch.full((b,), start_step-1, device=device, dtype=torch.long)\n",
    "                log_x_start = index_to_log_onehot(x_start, transformer.num_classes)\n",
    "\n",
    "                # t=T\n",
    "                zero_logits = torch.zeros((b, transformer.num_classes-1, transformer.shape),device=device)\n",
    "                one_logits = torch.ones((b, 1, transformer.shape),device=device)\n",
    "                mask_logits = torch.cat((zero_logits, one_logits), dim=1)\n",
    "                log_z = torch.log(mask_logits)\n",
    "\n",
    "                model_log_probs = []\n",
    "                log_zs = []\n",
    "                ts = []\n",
    "                vlb = []\n",
    "                for diffusion_index in diffusion_list:\n",
    "                    t = torch.full((b,), diffusion_index, device=device, dtype=torch.long)\n",
    "                    log_x_recon = transformer.cf_predict_start(log_z, cond_emb, t)\n",
    "                    log_zs.append(log_z)\n",
    "                    if diffusion_index > self.skip_step:\n",
    "                        model_log_prob = transformer.q_posterior(log_x_start=log_x_recon, log_x_t=log_z, t=t-self.skip_step)\n",
    "                        ts.append(t-self.skip_step)\n",
    "                    else:\n",
    "                        model_log_prob = transformer.q_posterior(log_x_start=log_x_recon, log_x_t=log_z, t=t)\n",
    "                        ts.append(t)\n",
    "\n",
    "                    model_log_probs.append(model_log_prob)\n",
    "                    log_z = transformer.log_sample_categorical(model_log_prob, noise=self.noise, noise_scale=self.noise_scale)\n",
    "\n",
    "                x_start = log_onehot_to_index(log_z)\n",
    "                log_x_start = index_to_log_onehot(x_start, transformer.num_classes)\n",
    "                for i, model_log_prob in enumerate(model_log_probs[:-1]):\n",
    "                    log_true_prob = transformer.q_posterior(log_x_start=log_x_start, log_x_t=log_zs[i], t=ts[i])\n",
    "                    kl = transformer.multinomial_kl(log_true_prob, model_log_prob)\n",
    "                    kl = sum_except_batch(kl).unsqueeze(1)\n",
    "                    vlb.append(-kl)\n",
    "\n",
    "                log_probs = model_log_probs[-1].permute(0, 2, 1)\n",
    "                target = F.one_hot(x_start, num_classes=transformer.num_classes)\n",
    "                rewards = (log_probs * target).sum(-1).sum(-1)\n",
    "                rewards += torch.concat(vlb, dim=1).sum(dim=1)\n",
    "                vlbs.append(rewards)\n",
    "        elif self.reward_type == 'likelihood':\n",
    "            # use observed samples for estimation\n",
    "            for diffusion_index in diffusion_list:\n",
    "                t = torch.full((b,), diffusion_index, device=device, dtype=torch.long)\n",
    "                log_x_recon = transformer.cf_predict_start(log_z, cond_emb, t)\n",
    "                if diffusion_index > self.skip_step:\n",
    "                    model_log_prob = transformer.q_posterior(log_x_start=log_x_recon, log_x_t=log_z, t=t-self.skip_step)\n",
    "                    log_true_prob = transformer.q_posterior(log_x_start=log_x_start, log_x_t=log_z, t=t-self.skip_step)\n",
    "                else:\n",
    "                    model_log_prob = transformer.q_posterior(log_x_start=log_x_recon, log_x_t=log_z, t=t)\n",
    "                    log_true_prob = transformer.q_posterior(log_x_start=log_x_start, log_x_t=log_z, t=t)\n",
    "\n",
    "                log_z = transformer.log_sample_categorical(model_log_prob, noise=self.noise, noise_scale=self.noise_scale)\n",
    "\n",
    "                # -KL if t !=0 else LL\n",
    "                if diffusion_index != 0:\n",
    "                    kl = transformer.multinomial_kl(log_true_prob, model_log_prob)\n",
    "                    kl = sum_except_batch(kl).unsqueeze(1)\n",
    "                    vlbs.append(-kl)\n",
    "                else:\n",
    "                    decoder_ll = log_categorical(log_x_start, model_log_prob)\n",
    "                    decoder_ll = sum_except_batch(decoder_ll).unsqueeze(1)   \n",
    "                    vlbs.append(decoder_ll)\n",
    "\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        rewards = torch.stack(vlbs, dim=1).mean(1)\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'overall': {'trainable': '34.8M', 'non_trainable': '0', 'total': '34.8M'}, 'content_codec': {'trainable': '17.62M', 'non_trainable': '0', 'total': '17.62M'}, 'transformer': {'trainable': '17.18M', 'non_trainable': '0', 'total': '17.18M'}}\n",
      "Model missing keys:\n",
      " []\n",
      "Model unexpected keys:\n",
      " []\n",
      "Evaluate EMA model\n"
     ]
    }
   ],
   "source": [
    "with open('/home/dongyoon/diffusion_reward/dongyoon/config/diffusion_reward.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "    config = SimpleNamespace(**config)\n",
    "reward_model = Custom_DiffusionReward(config)\n",
    "if torch.cuda.is_available():\n",
    "    reward_model = reward_model.to('cuda:6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pkl(pkl_path, indices):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    frames = []\n",
    "    for i in range(len(data['observations'])):\n",
    "        frame = data['observations'][i]['color_image2']\n",
    "        frame = np.transpose(frame, (1, 2, 0)) # chw -> hwc\n",
    "        img = Image.fromarray(frame)\n",
    "        resized_img = img.resize((64, 64))\n",
    "        frame = np.array(resized_img)\n",
    "        frames.append(frame)\n",
    "        \n",
    "    frames = np.array(frames)\n",
    "    \n",
    "    if indices is not None:\n",
    "        frames = frames[indices]\n",
    "    \n",
    "    frames = np.expand_dims(frames, axis=0) # dim 0 for batch\n",
    "    frames = frames.astype(np.float32)\n",
    "    frames = frames / 127.5 - 1 # normalize to [-1, 1]\n",
    "    frames = torch.from_numpy(frames).float().to('cuda:6')\n",
    "    return frames\n",
    "\n",
    "def pkl2frames(pkl_path):\n",
    "    with open(pkl_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    frames = []\n",
    "    for i in range(len(data['observations'])):\n",
    "        frame = data['observations'][i]['color_image2']\n",
    "        frame = np.transpose(frame, (1, 2, 0)) # chw -> hwc\n",
    "        img = Image.fromarray(frame)\n",
    "        resized_img = img.resize((64, 64))\n",
    "        frame = np.array(resized_img)\n",
    "        frames.append(frame)\n",
    "    frames = np.array(frames)\n",
    "    return frames\n",
    "\n",
    "def process_frames(frames):\n",
    "    frames = np.expand_dims(frames, axis=0) # dim 0 for batch\n",
    "    frames = frames.astype(np.float32)\n",
    "    frames = frames / 127.5 - 1 # normalize to [-1, 1]\n",
    "    frames = torch.from_numpy(frames).float().to('cuda:6')\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reward_100(combined_array, reward_model):\n",
    "    \"\"\"\n",
    "    combined_array: T * H * W * C\n",
    "    \"\"\"\n",
    "    reward_traj = np.zeros(0)\n",
    "    start_idx = 0\n",
    "    prev_last_idx = 0\n",
    "    last_idx = 100\n",
    "    while start_idx <= combined_array.shape[0]:\n",
    "        last_frame = min(last_idx, combined_array.shape[0])\n",
    "        if last_frame-start_idx < 20:\n",
    "            start_idx -= 20\n",
    "        selected_frames = combined_array[start_idx:last_frame]\n",
    "        frames = process_frames(selected_frames)\n",
    "        reward = reward_model.calc_reward(frames)\n",
    "        reward = reward.cpu().numpy().squeeze()\n",
    "        \n",
    "        reward = reward[prev_last_idx-start_idx:]\n",
    "        reward_traj = np.concatenate((reward_traj, reward))\n",
    "        \n",
    "        start_idx = last_idx - 20\n",
    "        prev_last_idx = last_idx\n",
    "        last_idx = start_idx + 100\n",
    "    return reward_traj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing: 0\n",
      "demolen: 483\n",
      "reward: 483\n",
      "processing: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m selected_frames \u001b[38;5;241m=\u001b[39m combined_array[start_idx:last_frame]\n\u001b[1;32m     27\u001b[0m frames \u001b[38;5;241m=\u001b[39m process_frames(selected_frames)\n\u001b[0;32m---> 28\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[43mreward_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_reward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     31\u001b[0m reward \u001b[38;5;241m=\u001b[39m reward[prev_last_idx\u001b[38;5;241m-\u001b[39mstart_idx:]\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 106\u001b[0m, in \u001b[0;36mCustom_DiffusionReward.calc_reward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m    103\u001b[0m content_token \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    104\u001b[0m condition_token \u001b[38;5;241m=\u001b[39m condition[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition_token\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 106\u001b[0m rewards \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalc_vlb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcondition_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_std:\n\u001b[1;32m    108\u001b[0m     rewards_std \u001b[38;5;241m=\u001b[39m (rewards \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstat[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 159\u001b[0m, in \u001b[0;36mCustom_DiffusionReward.calc_vlb\u001b[0;34m(self, cont_emb, cond_emb)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m diffusion_index \u001b[38;5;129;01min\u001b[39;00m diffusion_list:\n\u001b[1;32m    158\u001b[0m     t \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((b,), diffusion_index, device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m--> 159\u001b[0m     log_x_recon \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcf_predict_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_z\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     log_zs\u001b[38;5;241m.\u001b[39mappend(log_z)\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m diffusion_index \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_step:\n",
      "File \u001b[0;32m~/diffusion_reward/diffusion_reward/models/video_models/vqdiffusion/modeling/transformers/diffusion_transformer.py:228\u001b[0m, in \u001b[0;36mDiffusionTransformer.cf_predict_start\u001b[0;34m(self, log_x_t, cond_emb, t)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcf_predict_start\u001b[39m(\u001b[38;5;28mself\u001b[39m, log_x_t, cond_emb, t):\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_start\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_x_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusion_reward/diffusion_reward/models/video_models/vqdiffusion/modeling/transformers/diffusion_transformer.py:213\u001b[0m, in \u001b[0;36mDiffusionTransformer.predict_start\u001b[0;34m(self, log_x_t, cond_emb, t)\u001b[0m\n\u001b[1;32m    211\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(x_t, cond_emb, t)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 213\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m==\u001b[39m x_t\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diffusion_reward/diffusion_reward/models/video_models/vqdiffusion/modeling/transformers/transformer_utils.py:434\u001b[0m, in \u001b[0;36mText2ImageTransformer.forward\u001b[0;34m(self, input, cond_emb, t)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks)):   \n\u001b[1;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m--> 434\u001b[0m         emb, att_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mblock_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda:6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B x (Ld+Lt) x D, B x (Ld+Lt) x (Ld+Lt)\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m         emb, att_weight \u001b[38;5;241m=\u001b[39m checkpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks[block_idx], emb, cond_emb, t\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda:6\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diffusion_reward/diffusion_reward/models/video_models/vqdiffusion/modeling/transformers/transformer_utils.py:262\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, encoder_output, timestep, mask)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselfcross\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    261\u001b[0m     timestep \u001b[38;5;241m=\u001b[39m timestep\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 262\u001b[0m     a, att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m)\u001b[49m, encoder_output, mask\u001b[38;5;241m=\u001b[39mmask)\n\u001b[1;32m    263\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m a\n\u001b[1;32m    264\u001b[0m     a, att \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln1_1(x, timestep), encoder_output, mask\u001b[38;5;241m=\u001b[39mmask)\n",
      "File \u001b[0;32m~/anaconda3/envs/diffusion_reward/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/diffusion_reward/diffusion_reward/models/video_models/vqdiffusion/modeling/transformers/transformer_utils.py:146\u001b[0m, in \u001b[0;36mAdaLayerNorm.forward\u001b[0;34m(self, x, timestep)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, timestep):        \n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timestep[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiff_step:\n\u001b[1;32m    147\u001b[0m         _emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(timestep), \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    148\u001b[0m         emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msilu(_emb))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_set_path = '/home/dongyoon/diffusion_reward/video_dataset/furniture/low_oneleg/train'\n",
    "\n",
    "total_reward = np.zeros(0)\n",
    "subfolders = [f for f in sorted(os.listdir(train_set_path)) if f.isdigit()]\n",
    "total_reward = np.zeros(0)\n",
    "for folder in subfolders:\n",
    "    print(\"processing:\", folder)\n",
    "    folder_path = os.path.join(train_set_path, folder)\n",
    "    file_list = sorted([f for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "    \n",
    "    images = []\n",
    "    for file_name in file_list:\n",
    "        img_path = os.path.join(folder_path, file_name)\n",
    "        with Image.open(img_path) as img:\n",
    "            images.append(np.array(img))\n",
    "        \n",
    "    combined_array = np.stack(images)\n",
    "    reward_traj = np.zeros(0)\n",
    "    start_idx = 0\n",
    "    prev_last_idx = 0\n",
    "    last_idx = 100\n",
    "    while start_idx <= combined_array.shape[0]:\n",
    "        last_frame = min(last_idx, combined_array.shape[0])\n",
    "        if last_frame-start_idx < 20:\n",
    "            start_idx -= 20\n",
    "        selected_frames = combined_array[start_idx:last_frame]\n",
    "        frames = process_frames(selected_frames)\n",
    "        reward = reward_model.calc_reward(frames)\n",
    "        reward = reward.cpu().numpy().squeeze()\n",
    "        \n",
    "        reward = reward[prev_last_idx-start_idx:]\n",
    "        reward_traj = np.concatenate((reward_traj, reward))\n",
    "        \n",
    "        start_idx = last_idx - 20\n",
    "        prev_last_idx = last_idx\n",
    "        last_idx = start_idx + 100\n",
    "    print(\"demolen:\", combined_array.shape[0])\n",
    "    print(\"reward:\", len(reward_traj))\n",
    "    \n",
    "    total_reward = np.concatenate((total_reward, reward_traj))\n",
    "mean_reward = np.mean(total_reward)\n",
    "std_reward = np.std(total_reward)\n",
    "print(\"mean reward:\", mean_reward)\n",
    "print(\"std reward:\", std_reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_reward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
